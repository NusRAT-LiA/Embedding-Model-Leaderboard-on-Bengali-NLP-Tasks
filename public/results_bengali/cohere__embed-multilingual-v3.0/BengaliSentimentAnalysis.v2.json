{
  "dataset_revision": "23edb78a3dd297a4d92f9c011a0503be0c0949d0",
  "task_name": "BengaliSentimentAnalysis.v2",
  "mteb_version": "2.3.11",
  "scores": {
    "test": [
      {
        " ": [
          {
            "accuracy": 0.919434,
            "f1": 0.899121,
            "f1_weighted": 0.918793,
            "precision": 0.905805,
            "precision_weighted": 0.9186,
            "recall": 0.89309,
            "recall_weighted": 0.919434,
            "ap": 0.925158,
            "ap_weighted": 0.925158
          },
          {
            "accuracy": 0.932129,
            "f1": 0.918675,
            "f1_weighted": 0.93305,
            "precision": 0.908438,
            "precision_weighted": 0.935609,
            "recall": 0.931238,
            "recall_weighted": 0.932129,
            "ap": 0.95404,
            "ap_weighted": 0.95404
          },
          {
            "accuracy": 0.901855,
            "f1": 0.887346,
            "f1_weighted": 0.904915,
            "precision": 0.869995,
            "precision_weighted": 0.918885,
            "recall": 0.919553,
            "recall_weighted": 0.901855,
            "ap": 0.95036,
            "ap_weighted": 0.95036
          },
          {
            "accuracy": 0.906738,
            "f1": 0.888568,
            "f1_weighted": 0.908122,
            "precision": 0.878331,
            "precision_weighted": 0.911423,
            "recall": 0.901506,
            "recall_weighted": 0.906738,
            "ap": 0.933974,
            "ap_weighted": 0.933974
          },
          {
            "accuracy": 0.902344,
            "f1": 0.874844,
            "f1_weighted": 0.900339,
            "precision": 0.89229,
            "precision_weighted": 0.900977,
            "recall": 0.861296,
            "recall_weighted": 0.902344,
            "ap": 0.903653,
            "ap_weighted": 0.903653
          },
          {
            "accuracy": 0.89502,
            "f1": 0.87881,
            "f1_weighted": 0.898071,
            "precision": 0.862458,
            "precision_weighted": 0.910122,
            "recall": 0.907463,
            "recall_weighted": 0.89502,
            "ap": 0.941258,
            "ap_weighted": 0.941258
          },
          {
            "accuracy": 0.727539,
            "f1": 0.715096,
            "f1_weighted": 0.740971,
            "precision": 0.73121,
            "precision_weighted": 0.827304,
            "recall": 0.784962,
            "recall_weighted": 0.727539,
            "ap": 0.870731,
            "ap_weighted": 0.870731
          },
          {
            "accuracy": 0.938965,
            "f1": 0.925713,
            "f1_weighted": 0.939348,
            "precision": 0.920601,
            "precision_weighted": 0.940058,
            "recall": 0.931295,
            "recall_weighted": 0.938965,
            "ap": 0.952708,
            "ap_weighted": 0.952708
          },
          {
            "accuracy": 0.89209,
            "f1": 0.871066,
            "f1_weighted": 0.893691,
            "precision": 0.861314,
            "precision_weighted": 0.897165,
            "recall": 0.883447,
            "recall_weighted": 0.89209,
            "ap": 0.921915,
            "ap_weighted": 0.921915
          },
          {
            "accuracy": 0.907715,
            "f1": 0.889421,
            "f1_weighted": 0.908967,
            "precision": 0.879931,
            "precision_weighted": 0.911797,
            "recall": 0.90114,
            "recall_weighted": 0.907715,
            "ap": 0.9335,
            "ap_weighted": 0.9335
          }
        ],
        "accuracy": 0.892383,
        "f1": 0.874866,
        "f1_weighted": 0.894627,
        "precision": 0.871037,
        "precision_weighted": 0.907194,
        "recall": 0.891499,
        "recall_weighted": 0.892383,
        "ap": 0.92873,
        "ap_weighted": 0.92873,
        "main_score": 0.874866
      }
    ]
  },
  "evaluation_time": 43.83061623573303,
  "kg_co2_emissions": null
}