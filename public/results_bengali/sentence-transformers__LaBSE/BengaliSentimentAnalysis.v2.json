{
  "dataset_revision": "23edb78a3dd297a4d92f9c011a0503be0c0949d0",
  "task_name": "BengaliSentimentAnalysis.v2",
  "mteb_version": "2.3.11",
  "scores": {
    "test": [
      {
        " ": [
          {
            "accuracy": 0.848145,
            "f1": 0.816091,
            "f1_weighted": 0.849457,
            "precision": 0.811003,
            "precision_weighted": 0.851315,
            "recall": 0.821945,
            "recall_weighted": 0.848145,
            "ap": 0.881804,
            "ap_weighted": 0.881804
          },
          {
            "accuracy": 0.861328,
            "f1": 0.836943,
            "f1_weighted": 0.864346,
            "precision": 0.82502,
            "precision_weighted": 0.871582,
            "recall": 0.854679,
            "recall_weighted": 0.861328,
            "ap": 0.904227,
            "ap_weighted": 0.904227
          },
          {
            "accuracy": 0.839844,
            "f1": 0.816825,
            "f1_weighted": 0.845044,
            "precision": 0.803037,
            "precision_weighted": 0.861325,
            "recall": 0.845458,
            "recall_weighted": 0.839844,
            "ap": 0.900344,
            "ap_weighted": 0.900344
          },
          {
            "accuracy": 0.852051,
            "f1": 0.826722,
            "f1_weighted": 0.855512,
            "precision": 0.814607,
            "precision_weighted": 0.863904,
            "recall": 0.845596,
            "recall_weighted": 0.852051,
            "ap": 0.898635,
            "ap_weighted": 0.898635
          },
          {
            "accuracy": 0.813965,
            "f1": 0.783245,
            "f1_weighted": 0.818706,
            "precision": 0.772398,
            "precision_weighted": 0.829138,
            "recall": 0.801782,
            "recall_weighted": 0.813965,
            "ap": 0.871364,
            "ap_weighted": 0.871364
          },
          {
            "accuracy": 0.816895,
            "f1": 0.79809,
            "f1_weighted": 0.824868,
            "precision": 0.787586,
            "precision_weighted": 0.860043,
            "recall": 0.843587,
            "recall_weighted": 0.816895,
            "ap": 0.902685,
            "ap_weighted": 0.902685
          },
          {
            "accuracy": 0.560547,
            "f1": 0.557982,
            "f1_weighted": 0.572614,
            "precision": 0.632418,
            "precision_weighted": 0.743084,
            "recall": 0.650244,
            "recall_weighted": 0.560547,
            "ap": 0.792644,
            "ap_weighted": 0.792644
          },
          {
            "accuracy": 0.825684,
            "f1": 0.802099,
            "f1_weighted": 0.831788,
            "precision": 0.789009,
            "precision_weighted": 0.8513,
            "recall": 0.832971,
            "recall_weighted": 0.825684,
            "ap": 0.89282,
            "ap_weighted": 0.89282
          },
          {
            "accuracy": 0.834961,
            "f1": 0.80042,
            "f1_weighted": 0.836502,
            "precision": 0.795239,
            "precision_weighted": 0.83867,
            "recall": 0.806477,
            "recall_weighted": 0.834961,
            "ap": 0.872412,
            "ap_weighted": 0.872412
          },
          {
            "accuracy": 0.79834,
            "f1": 0.76701,
            "f1_weighted": 0.804139,
            "precision": 0.756296,
            "precision_weighted": 0.817438,
            "recall": 0.787751,
            "recall_weighted": 0.79834,
            "ap": 0.863229,
            "ap_weighted": 0.863229
          }
        ],
        "accuracy": 0.805176,
        "f1": 0.780543,
        "f1_weighted": 0.810297,
        "precision": 0.778661,
        "precision_weighted": 0.83878,
        "recall": 0.809049,
        "recall_weighted": 0.805176,
        "ap": 0.878016,
        "ap_weighted": 0.878016,
        "main_score": 0.780543
      }
    ]
  },
  "evaluation_time": 26.615045070648193,
  "kg_co2_emissions": null
}