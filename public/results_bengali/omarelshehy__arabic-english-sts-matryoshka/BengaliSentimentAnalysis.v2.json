{
  "dataset_revision": "23edb78a3dd297a4d92f9c011a0503be0c0949d0",
  "task_name": "BengaliSentimentAnalysis.v2",
  "mteb_version": "2.3.11",
  "scores": {
    "test": [
      {
        " ": [
          {
            "accuracy": 0.868164,
            "f1": 0.837809,
            "f1_weighted": 0.868301,
            "precision": 0.837112,
            "precision_weighted": 0.868446,
            "recall": 0.838516,
            "recall_weighted": 0.868164,
            "ap": 0.891315,
            "ap_weighted": 0.891315
          },
          {
            "accuracy": 0.911621,
            "f1": 0.897115,
            "f1_weighted": 0.913904,
            "precision": 0.881015,
            "precision_weighted": 0.923051,
            "recall": 0.922698,
            "recall_weighted": 0.911621,
            "ap": 0.950972,
            "ap_weighted": 0.950972
          },
          {
            "accuracy": 0.861328,
            "f1": 0.845688,
            "f1_weighted": 0.867037,
            "precision": 0.830927,
            "precision_weighted": 0.896282,
            "recall": 0.891302,
            "recall_weighted": 0.861328,
            "ap": 0.934405,
            "ap_weighted": 0.934405
          },
          {
            "accuracy": 0.90625,
            "f1": 0.889764,
            "f1_weighted": 0.90829,
            "precision": 0.875769,
            "precision_weighted": 0.914858,
            "recall": 0.910059,
            "recall_weighted": 0.90625,
            "ap": 0.941196,
            "ap_weighted": 0.941196
          },
          {
            "accuracy": 0.861328,
            "f1": 0.826078,
            "f1_weighted": 0.860105,
            "precision": 0.832195,
            "precision_weighted": 0.859374,
            "recall": 0.820672,
            "recall_weighted": 0.861328,
            "ap": 0.879516,
            "ap_weighted": 0.879516
          },
          {
            "accuracy": 0.867188,
            "f1": 0.851376,
            "f1_weighted": 0.872442,
            "precision": 0.835794,
            "precision_weighted": 0.898441,
            "recall": 0.89434,
            "recall_weighted": 0.867188,
            "ap": 0.935787,
            "ap_weighted": 0.935787
          },
          {
            "accuracy": 0.877441,
            "f1": 0.857928,
            "f1_weighted": 0.880809,
            "precision": 0.843071,
            "precision_weighted": 0.89178,
            "recall": 0.883176,
            "recall_weighted": 0.877441,
            "ap": 0.924229,
            "ap_weighted": 0.924229
          },
          {
            "accuracy": 0.907227,
            "f1": 0.88857,
            "f1_weighted": 0.908384,
            "precision": 0.879773,
            "precision_weighted": 0.910859,
            "recall": 0.89923,
            "recall_weighted": 0.907227,
            "ap": 0.93203,
            "ap_weighted": 0.93203
          },
          {
            "accuracy": 0.890137,
            "f1": 0.870296,
            "f1_weighted": 0.892341,
            "precision": 0.857847,
            "precision_weighted": 0.898209,
            "recall": 0.88784,
            "recall_weighted": 0.890137,
            "ap": 0.925776,
            "ap_weighted": 0.925776
          },
          {
            "accuracy": 0.877441,
            "f1": 0.855177,
            "f1_weighted": 0.879854,
            "precision": 0.843456,
            "precision_weighted": 0.8857,
            "recall": 0.871666,
            "recall_weighted": 0.877441,
            "ap": 0.914967,
            "ap_weighted": 0.914967
          }
        ],
        "accuracy": 0.882812,
        "f1": 0.86198,
        "f1_weighted": 0.885147,
        "precision": 0.851696,
        "precision_weighted": 0.8947,
        "recall": 0.88195,
        "recall_weighted": 0.882812,
        "ap": 0.923019,
        "ap_weighted": 0.923019,
        "main_score": 0.86198
      }
    ]
  },
  "evaluation_time": 29.416731119155884,
  "kg_co2_emissions": null
}