{
  "dataset_revision": "23edb78a3dd297a4d92f9c011a0503be0c0949d0",
  "task_name": "BengaliSentimentAnalysis.v2",
  "mteb_version": "2.3.11",
  "scores": {
    "test": [
      {
        " ": [
          {
            "accuracy": 0.723633,
            "f1": 0.674454,
            "f1_weighted": 0.72944,
            "precision": 0.669011,
            "precision_weighted": 0.738261,
            "recall": 0.683879,
            "recall_weighted": 0.723633,
            "ap": 0.803411,
            "ap_weighted": 0.803411
          },
          {
            "accuracy": 0.85498,
            "f1": 0.835107,
            "f1_weighted": 0.859984,
            "precision": 0.82014,
            "precision_weighted": 0.878426,
            "recall": 0.868043,
            "recall_weighted": 0.85498,
            "ap": 0.91587,
            "ap_weighted": 0.91587
          },
          {
            "accuracy": 0.817871,
            "f1": 0.796977,
            "f1_weighted": 0.825281,
            "precision": 0.785106,
            "precision_weighted": 0.853881,
            "recall": 0.83642,
            "recall_weighted": 0.817871,
            "ap": 0.896734,
            "ap_weighted": 0.896734
          },
          {
            "accuracy": 0.775391,
            "f1": 0.756243,
            "f1_weighted": 0.785932,
            "precision": 0.752017,
            "precision_weighted": 0.832646,
            "recall": 0.805761,
            "recall_weighted": 0.775391,
            "ap": 0.879376,
            "ap_weighted": 0.879376
          },
          {
            "accuracy": 0.751953,
            "f1": 0.703206,
            "f1_weighted": 0.755477,
            "precision": 0.698289,
            "precision_weighted": 0.760316,
            "recall": 0.709899,
            "recall_weighted": 0.751953,
            "ap": 0.8171,
            "ap_weighted": 0.8171
          },
          {
            "accuracy": 0.835938,
            "f1": 0.818059,
            "f1_weighted": 0.842844,
            "precision": 0.80544,
            "precision_weighted": 0.87433,
            "recall": 0.862616,
            "recall_weighted": 0.835938,
            "ap": 0.91493,
            "ap_weighted": 0.91493
          },
          {
            "accuracy": 0.634277,
            "f1": 0.629565,
            "f1_weighted": 0.647722,
            "precision": 0.687893,
            "precision_weighted": 0.799382,
            "recall": 0.721521,
            "recall_weighted": 0.634277,
            "ap": 0.835581,
            "ap_weighted": 0.835581
          },
          {
            "accuracy": 0.900391,
            "f1": 0.881593,
            "f1_weighted": 0.902095,
            "precision": 0.870271,
            "precision_weighted": 0.906398,
            "recall": 0.896558,
            "recall_weighted": 0.900391,
            "ap": 0.931063,
            "ap_weighted": 0.931063
          },
          {
            "accuracy": 0.80127,
            "f1": 0.777689,
            "f1_weighted": 0.809153,
            "precision": 0.766698,
            "precision_weighted": 0.835997,
            "recall": 0.813337,
            "recall_weighted": 0.80127,
            "ap": 0.881374,
            "ap_weighted": 0.881374
          },
          {
            "accuracy": 0.805664,
            "f1": 0.776482,
            "f1_weighted": 0.811579,
            "precision": 0.765066,
            "precision_weighted": 0.826307,
            "recall": 0.799658,
            "recall_weighted": 0.805664,
            "ap": 0.870792,
            "ap_weighted": 0.870792
          }
        ],
        "accuracy": 0.790137,
        "f1": 0.764937,
        "f1_weighted": 0.796951,
        "precision": 0.761993,
        "precision_weighted": 0.830595,
        "recall": 0.799769,
        "recall_weighted": 0.790137,
        "ap": 0.874623,
        "ap_weighted": 0.874623,
        "main_score": 0.764937
      }
    ]
  },
  "evaluation_time": 29.195921659469604,
  "kg_co2_emissions": null
}